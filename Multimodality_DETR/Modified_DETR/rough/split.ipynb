{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #****************************train-> train and val (visible)*****************************\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to the dataset\n",
    "# # dataset_path = \"/data/Multimodal_DETR_v1/rough/dataset_visible/train\"\n",
    "# dataset_path = \"/data/Singlemodality_DETR_V1/dataset_visible/train/\"\n",
    "\n",
    "# # Load the original annotations\n",
    "# with open(os.path.join(dataset_path, \"_annotations.coco.json\")) as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Create directories for the training and validation sets\n",
    "# os.makedirs(os.path.join(dataset_path, \"train_set\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(dataset_path, \"val_set\"), exist_ok=True)\n",
    "\n",
    "# # Get a list of all image ids\n",
    "# image_ids = [image[\"id\"] for image in data[\"images\"]]\n",
    "\n",
    "# # Randomly shuffle the image ids\n",
    "# np.random.shuffle(image_ids)\n",
    "\n",
    "# # Split the image ids into training and validation sets\n",
    "# train_size = int(0.82 * len(image_ids))\n",
    "# train_image_ids = image_ids[:train_size]\n",
    "# val_image_ids = image_ids[train_size:]\n",
    "\n",
    "# # Function to create a new annotation file\n",
    "# def create_annotation_file(image_ids, set_name):\n",
    "#     # Filter the images and annotations\n",
    "#     images = [image for image in data[\"images\"] if image[\"id\"] in image_ids]\n",
    "#     annotations = [ann for ann in data[\"annotations\"] if ann[\"image_id\"] in image_ids]\n",
    "\n",
    "#     # Create the new data\n",
    "#     new_data = {\n",
    "#         \"info\": data[\"info\"],\n",
    "#         \"info_improved\": data[\"info_improved\"],\n",
    "#         \"images\": images,\n",
    "#         \"annotations\": annotations,\n",
    "#         \"categories\": data[\"categories\"]\n",
    "#     }\n",
    "\n",
    "#     # Write the new data to a JSON file\n",
    "#     with open(os.path.join(dataset_path, f\"{set_name}_set\", \"_annotations.coco.json\"), 'w') as f:\n",
    "#         json.dump(new_data, f)\n",
    "\n",
    "#     # Copy the corresponding images to the new directory\n",
    "#     for image in images:\n",
    "#         shutil.copy(os.path.join(dataset_path, image[\"file_name\"]), os.path.join(dataset_path, f\"{set_name}_set\", image[\"file_name\"]))\n",
    "\n",
    "# # Create the annotation files\n",
    "# create_annotation_file(train_image_ids, \"train\")\n",
    "# create_annotation_file(val_image_ids, \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #****************************train-> train and val (lwir)*****************************\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to the dataset\n",
    "# # dataset_path = \"/data/Multimodal_DETR_v1/rough/dataset_visible/train\"\n",
    "# dataset_path = \"/data/Singlemodality_DETR_V1/dataset_lwir/train/\"\n",
    "\n",
    "# # Load the original annotations\n",
    "# with open(os.path.join(dataset_path, \"_annotations.coco.json\")) as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Create directories for the training and validation sets\n",
    "# os.makedirs(os.path.join(dataset_path, \"train_set\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(dataset_path, \"val_set\"), exist_ok=True)\n",
    "\n",
    "# # Get a list of all image ids\n",
    "# image_ids = [image[\"id\"] for image in data[\"images\"]]\n",
    "\n",
    "# # Randomly shuffle the image ids\n",
    "# np.random.shuffle(image_ids)\n",
    "\n",
    "# # Split the image ids into training and validation sets\n",
    "# train_size = int(0.82 * len(image_ids))\n",
    "# train_image_ids = image_ids[:train_size]\n",
    "# val_image_ids = image_ids[train_size:]\n",
    "\n",
    "# # Function to create a new annotation file\n",
    "# def create_annotation_file(image_ids, set_name):\n",
    "#     # Filter the images and annotations\n",
    "#     images = [image for image in data[\"images\"] if image[\"id\"] in image_ids]\n",
    "#     annotations = [ann for ann in data[\"annotations\"] if ann[\"image_id\"] in image_ids]\n",
    "\n",
    "#     # Create the new data\n",
    "#     new_data = {\n",
    "#         \"info\": data[\"info\"],\n",
    "#         \"info_improved\": data[\"info_improved\"],\n",
    "#         \"images\": images,\n",
    "#         \"annotations\": annotations,\n",
    "#         \"categories\": data[\"categories\"]\n",
    "#     }\n",
    "\n",
    "#     # Write the new data to a JSON file\n",
    "#     with open(os.path.join(dataset_path, f\"{set_name}_set\", \"_annotations.coco.json\"), 'w') as f:\n",
    "#         json.dump(new_data, f)\n",
    "\n",
    "#     # Copy the corresponding images to the new directory\n",
    "#     for image in images:\n",
    "#         shutil.copy(os.path.join(dataset_path, image[\"file_name\"]), os.path.join(dataset_path, f\"{set_name}_set\", image[\"file_name\"]))\n",
    "\n",
    "# # Create the annotation files\n",
    "# create_annotation_file(train_image_ids, \"train\")\n",
    "# create_annotation_file(val_image_ids, \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************train-> train and val (both modality)*****************************\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the datasets\n",
    "dataset_path1 = \"/data/DETR/dataset_visible/train/\"\n",
    "dataset_path2 = \"/data/DETR/dataset_lwir/train/\"\n",
    "\n",
    "# Load the original annotations\n",
    "with open(os.path.join(dataset_path1, \"train_annotations.json\")) as f:\n",
    "    data1 = json.load(f)\n",
    "with open(os.path.join(dataset_path2, \"train_annotations.json\")) as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# Create directories for the training and validation sets\n",
    "for dataset_path in [dataset_path1, dataset_path2]:\n",
    "    os.makedirs(os.path.join(dataset_path, \"train_set\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_path, \"val_set\"), exist_ok=True)\n",
    "\n",
    "# Get a list of all image ids\n",
    "image_ids1 = [image[\"id\"] for image in data1[\"images\"]]\n",
    "image_ids2 = [image[\"id\"] for image in data2[\"images\"]]\n",
    "\n",
    "# Check if image_ids are similar\n",
    "if set(image_ids1) != set(image_ids2):\n",
    "    raise ValueError(\"The image ids in the two datasets do not match.\")\n",
    "\n",
    "# Randomly shuffle the image ids\n",
    "np.random.shuffle(image_ids1)\n",
    "\n",
    "# Split the image ids into training and validation sets\n",
    "train_size = int(0.82 * len(image_ids1))\n",
    "train_image_ids = image_ids1[:train_size]\n",
    "val_image_ids = image_ids1[train_size:]\n",
    "\n",
    "# Function to create a new annotation file\n",
    "def create_annotation_file(image_ids, set_name, dataset_path, data):\n",
    "    # Filter the images and annotations\n",
    "    images = [image for image in data[\"images\"] if image[\"id\"] in image_ids]\n",
    "    annotations = [ann for ann in data[\"annotations\"] if ann[\"image_id\"] in image_ids]\n",
    "\n",
    "    # Create the new data\n",
    "    new_data = {\n",
    "        \"info\": data[\"info\"],\n",
    "        \"info_improved\": data[\"info_improved\"],\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    # Write the new data to a JSON file\n",
    "    with open(os.path.join(dataset_path, f\"{set_name}_set\", \"annotations.json\"), 'w') as f:\n",
    "        json.dump(new_data, f)\n",
    "\n",
    "    # Copy the corresponding images to the new directory\n",
    "    for image in images:\n",
    "        shutil.copy(os.path.join(dataset_path, image[\"file_name\"]), os.path.join(dataset_path, f\"{set_name}_set\", image[\"file_name\"]))\n",
    "\n",
    "# Create the annotation files for both datasets\n",
    "for dataset_path, data in [(dataset_path1, data1), (dataset_path2, data2)]:\n",
    "    create_annotation_file(train_image_ids, \"train\", dataset_path, data)\n",
    "    create_annotation_file(val_image_ids, \"val\", dataset_path, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
